
# ğŸ¤– LexiRAG â€“ Document Intelligence Platform with RAG

LexiRAG is a full-stack AI-powered platform that allows users to upload documents (PDF, DOCX, TXT) and interact with them via natural language questions. It uses a **Retrieval-Augmented Generation (RAG)** pipeline to generate accurate, contextual answers based only on the content of the selected document.

---

## ğŸš€ Features

- ğŸ“¤ Upload PDF, DOCX, or TXT files
- ğŸ“ Document library with metadata and processing status
- ğŸ’¬ Chat interface to ask document-based questions
- ğŸ§  RAG pipeline:
  - Text parsing & chunking
  - Embedding via Sentence Transformers
  - Vector similarity search using ChromaDB
  - Answer generation via Mistral-7B using OpenRouter
- ğŸ’¡ Answers are generated using only the relevant chunks of the selected document
- ğŸ¨ Beautiful dark-themed UI built with React, Tailwind CSS, and Shadcn UI

---

## ğŸ› ï¸ Tech Stack

| Layer       | Technology                                |
|-------------|--------------------------------------------|
| Frontend    | React, Vite, Tailwind CSS, Shadcn UI       |
| Backend     | Django REST Framework                      |
| Vector Store| ChromaDB                                   |
| Embeddings  | Sentence Transformers (all-MiniLM-L6-v2)   |
| LLM API     | Mistral-7B via OpenRouter                  |
| Parsing     | pdfplumber, python-docx                    |
| Storage     | Local file system                          |

---

## âš™ï¸ Installation & Setup

### ğŸ”§ Backend Setup

```bash
git clone https://github.com/your-username/lexirag.git
cd lexirag/backend

python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

pip install -r requirements.txt

# Create a .env file
echo "OPENROUTER_API_KEY=your_key_here" > .env

python manage.py runserver
```

### ğŸ’» Frontend Setup

```bash
cd ../frontend

npm install
npm run dev
```

---

## ğŸ§ª API Endpoints

### ğŸ“¥ Upload a Document

```http
POST /api/documents/upload/
Content-Type: multipart/form-data
Body: file=<document_file>
```

### ğŸ’¬ Ask a Question

```http
POST /api/query/
Content-Type: application/json

{
  "document_id": 1,
  "question": "What is this document about?"
}
```

---

## ğŸ“‚ Project Structure

```
lexirag/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ views.py, models.py, urls.py
â”‚   â”‚   â””â”€â”€ rag/
â”‚   â”‚       â”œâ”€â”€ parser.py
â”‚   â”‚       â”œâ”€â”€ chunker.py
â”‚   â”‚       â”œâ”€â”€ embedder.py
â”‚   â”‚       â”œâ”€â”€ vector_store.py
â”‚   â”‚       â””â”€â”€ generator.py
â”‚   â””â”€â”€ media/
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ pages/
    â”‚   â”‚   â”œâ”€â”€ LandingPage.jsx
    â”‚   â”‚   â”œâ”€â”€ UploadPage.jsx
    â”‚   â”‚   â”œâ”€â”€ LibraryPage.jsx
    â”‚   â”‚   â””â”€â”€ ChatPage.jsx
    â”‚   â””â”€â”€ components/
    â”‚       â”œâ”€â”€ Navbar.jsx
    â”‚       â””â”€â”€ DocumentCard.jsx
    â””â”€â”€ index.html, tailwind.config.js
```

---

## ğŸ§  RAG Pipeline

1. Document is parsed â†’ chunked â†’ embedded.
2. Embeddings are stored in ChromaDB with metadata.
3. Query is embedded and matched to document-specific vectors.
4. Top-k chunks are used as context in a guided prompt.
5. Mistral-7B responds with a contextual, non-repetitive answer.

---

## ğŸ“¸ Screenshots

> _Add screenshots of:_
> - Landing Page  
> - Upload Page  
> - Document Library  
> - Chat Interface  

---

## ğŸ“œ License

This project is intended for academic and demonstration purposes. For production use or licensing, contact the author.

---

## ğŸ™‹â€â™‚ï¸ Author

**Kartik Ambupe**  
ğŸ“ Mumbai / Kolhapur  
ğŸ“ Somaiya Vidyavihar University  
